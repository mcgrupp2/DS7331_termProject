{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = [\n",
    "    'age',\n",
    "    'class_worker',\n",
    "    'det_ind_code',\n",
    "    'det_occ_code',\n",
    "    'education',\n",
    "    'wage_per_hour',\n",
    "    'hs_college',\n",
    "    'marital_stat',\n",
    "    'major_ind_code',\n",
    "    'major_occ_code',\n",
    "    'race',\n",
    "    'hisp_origin',\n",
    "    'sex',\n",
    "    'union_member',\n",
    "    'unemp_reason',\n",
    "    'full_or_part_emp',\n",
    "    'capital_gains',\n",
    "    'capital_losses',\n",
    "    'stock_dividends',\n",
    "    'tax_filer_stat',\n",
    "    'region_prev_res',\n",
    "    'state_prev_res',\n",
    "    'det_hh_fam_stat',\n",
    "    'det_hh_summ',\n",
    "    'instance_weight', ## this field is not used as a feature\n",
    "    'mig_chg_msa',\n",
    "    'mig_chg_reg',\n",
    "    'mig_move_reg',\n",
    "    'mig_same',\n",
    "    'mig_prev_sunbelt',\n",
    "    'num_emp',\n",
    "    'fam_under_18',\n",
    "    'country_father',\n",
    "    'country_mother',\n",
    "    'country_self',\n",
    "    'citizenship',\n",
    "    'own_or_self',\n",
    "    'vet_question',\n",
    "    'vet_benefits',\n",
    "    'weeks_worked',\n",
    "    'year',\n",
    "    'income_50k',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"../input/ml1-project/census-income.data.csv\",header = None, names = header_names)\n",
    "df2=pd.read_csv(\"../input/testset/census-income.test.csv\",header = None, names = header_names)\n",
    "\n",
    "df = pd.concat([df1, df2]) #The test file, labeled so it can be merged with original \n",
    "df.drop(columns = ['instance_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new variable for classification based of if the person recieved a \n",
    "## college degree\n",
    "higer_degrees = [\n",
    "    ' Bachelors degree(BA AB BS)', \n",
    "    ' Masters degree(MA MS MEng MEd MSW MBA)', \n",
    "    ' Prof school degree (MD DDS DVM LLB JD)',\n",
    "    ' Doctorate degree(PhD EdD)',\n",
    "]\n",
    "\n",
    "df['graduated'] = 'no'\n",
    "df.loc[df['education'].isin(higer_degrees), 'graduated'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.head()\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"capital_gains\", y=\"age\", hue=\"income_50k\", alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"capital_losses\", y=\"age\", hue=\"income_50k\", alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"stock_dividends\", y=\"age\", hue=\"income_50k\", alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep=[\n",
    "    'age', \n",
    "    'education', \n",
    "    'race', \n",
    "    'sex', \n",
    "    'capital_gains', \n",
    "    'capital_losses', \n",
    "    'stock_dividends', \n",
    "    'tax_filer_stat', \n",
    "    'det_hh_summ', \n",
    "    'own_or_self', \n",
    "    'vet_benefits', \n",
    "    'weeks_worked',\n",
    "    'income_50k'\n",
    "]\n",
    "\n",
    "df_trunc = df.loc[:,cols_to_keep]\n",
    "\n",
    "df_trunc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_cols=['education', 'race', 'sex', 'tax_filer_stat', 'det_hh_summ']\n",
    "\n",
    "df_trunc.loc[:,ind_cols].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding \n",
    "tmp_df = pd.get_dummies(df_trunc.loc[:,ind_cols])\n",
    "df_trunc=df_trunc.drop(['education', 'race', 'sex', 'tax_filer_stat', 'det_hh_summ'], axis=1)\n",
    "df_trunc_ind = pd.concat((df_trunc,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "list(df_trunc_ind.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"own_or_self\", \"vet_benefits\", \"weeks_worked\", \"income_50k\"]:\n",
    "    df_trunc_ind[col] = df_trunc_ind[col].astype('category')\n",
    "    \n",
    "df_trunc_ind.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df_trunc_ind['income_50k']\n",
    "\n",
    "X = df_trunc_ind.drop('income_50k', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# NEED to make pipeline or columntransformer \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = df_trunc_ind.copy()\n",
    "\n",
    "cols_to_scale = ['capital_gains', 'capital_losses', 'stock_dividends']\n",
    "\n",
    "features = df_scaled[cols_to_scale]\n",
    "\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "df_scaled[cols_to_scale] = features\n",
    "\n",
    "df_scaled.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=100000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, lr.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "# print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# \n",
    "# svc = SVC() #MAKE SURE TO ADD KERNEL\n",
    "# \n",
    "# # Fit the model to the training data\n",
    "# svc.fit(X_train, y_train)\n",
    "# \n",
    "# # Calculate accuracy scores on both train and test data\n",
    "# accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "# accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "# \n",
    "# print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# # Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "# rfe = RFE(estimator=LogisticRegression(), n_features_to_select=10, verbose=1)\n",
    "# \n",
    "# # Fits the eliminator to the data\n",
    "# rfe.fit(X_train, y_train)\n",
    "# \n",
    "# # Print the features and their ranking (high = dropped early on)\n",
    "# print(dict(zip(X.columns, rfe.ranking_)))\n",
    "# \n",
    "# # Print the features that are not eliminated\n",
    "# print(X.columns[rfe.support_])\n",
    "# \n",
    "# # Calculates the test set accuracy\n",
    "# acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "# print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
