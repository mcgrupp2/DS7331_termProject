{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"header_names = [\n    'age',\n    'class_worker',\n    'det_ind_code',\n    'det_occ_code',\n    'education',\n    'wage_per_hour',\n    'hs_college',\n    'marital_stat',\n    'major_ind_code',\n    'major_occ_code',\n    'race',\n    'hisp_origin',\n    'sex',\n    'union_member',\n    'unemp_reason',\n    'full_or_part_emp',\n    'capital_gains',\n    'capital_losses',\n    'stock_dividends',\n    'tax_filer_stat',\n    'region_prev_res',\n    'state_prev_res',\n    'det_hh_fam_stat',\n    'det_hh_summ',\n    'instance_weight', ## this field is not used as a feature\n    'mig_chg_msa',\n    'mig_chg_reg',\n    'mig_move_reg',\n    'mig_same',\n    'mig_prev_sunbelt',\n    'num_emp',\n    'fam_under_18',\n    'country_father',\n    'country_mother',\n    'country_self',\n    'citizenship',\n    'own_or_self',\n    'vet_question',\n    'vet_benefits',\n    'weeks_worked',\n    'year',\n    'income_50k',\n]\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1=pd.read_csv(\"../input/ml1-project/census-income.data.csv\",header = None, names = header_names)\ndf2=pd.read_csv(\"../input/testset/census-income.test.csv\",header = None, names = header_names)\n\ndf = pd.concat([df1, df2]) #The test file, labeled so it can be merged with original \ndf.drop(columns = ['instance_weight'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Create a new variable for classification based of if the person recieved a \n# ## college degree\n# higer_degrees = [\n#     ' Bachelors degree(BA AB BS)', \n#     ' Masters degree(MA MS MEng MEd MSW MBA)', \n#     ' Prof school degree (MD DDS DVM LLB JD)',\n#     ' Doctorate degree(PhD EdD)',\n# ]\n# \n# df['graduated'] = 'no'\n# df.loc[df['education'].isin(higer_degrees), 'graduated'] = 'yes'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# df.shape\n# df.head()\n# list(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.scatterplot(data=df, x=\"capital_gains\", y=\"age\", hue=\"income_50k\", alpha=.4)# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.scatterplot(data=df, x=\"capital_losses\", y=\"age\", hue=\"income_50k\", alpha=.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.scatterplot(data=df, x=\"stock_dividends\", y=\"age\", hue=\"income_50k\", alpha=.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_keep=[\n    'age', \n    'education', \n    'race', \n    'sex', \n    'capital_gains', \n    'capital_losses', \n    'stock_dividends', \n    'tax_filer_stat', \n    'det_hh_summ', \n    'own_or_self', \n    'vet_benefits', \n    'weeks_worked',\n    'income_50k'\n]\n\ndf_trunc = df.loc[:,cols_to_keep]\n\ndf_trunc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trunc.loc[df_trunc.income_50k == \" - 50000.\", 'income_50k'] = \"below_50k\"\n\ndf_trunc.loc[df_trunc.income_50k == \" 50000+.\", 'income_50k'] = \"above_50k\"\n\n# df_trunc[\"income_50k\"].unique()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_cols=['education', 'race', 'sex', 'tax_filer_stat', 'det_hh_summ', \"own_or_self\", \"vet_benefits\", \"income_50k\"]\n\n# df_trunc.loc[:,ind_cols].head()\n\nfor col in ind_cols:\n    df_trunc[col] = df_trunc[col].astype('category')\n    \n# df_trunc.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"tmp_df = pd.get_dummies(df_trunc['income_50k'])\n\ndf_trunc = pd.concat((df_trunc,tmp_df),axis=1)\n\ndf_trunc=df_trunc.drop(['income_50k', 'below_50k'], axis=1)\n\nprint(list(df_trunc.columns))\n\nprint(df_trunc.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_trunc['above_50k']\n\nX = df_trunc.drop('above_50k', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\ncols_to_scale = ['capital_gains', 'capital_losses', 'stock_dividends']\n\nscaler = StandardScaler()\n\n# categorical_features = ['embarked', 'sex', 'pclass']\ncat_cols = ind_cols[:-1]\n\none_encode = OneHotEncoder()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', scaler, cols_to_scale),\n        ('cat', one_encode, cat_cols)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression(max_iter=100000))])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nclf.fit(X_train, y_train)\nprint(\"model score: \" + str(round((clf.score(X_test, y_test) * 100),2)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics as mt\n\ny_hat = clf.predict(X_test) \n\nacc = mt.accuracy_score(y_test,y_hat)\n# prec = mt.precision_score(y_test,y_hat)\n# recall = mt.recall_score(y_test,y_hat)\nconf = mt.confusion_matrix(y_test,y_hat)\n\n\n\nprint(\"accuracy\", acc )\n# print(\"precision\",prec)\n# print(\"recall\",recall)\nprint(\"confusion matrix\\n\",conf)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = mt.roc_curve(y_test, y_hat, pos_label=1)\nmt.auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\n\n# Fit the model to the training data\nsvc.fit(X_train, y_train)\n\n# Calculate accuracy scores on both train and test data\naccuracy_train = accuracy_score(y_train, svc.predict(X_train))\naccuracy_test = accuracy_score(y_test, svc.predict(X_test))\n\nprint(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_selection import RFE\n# # Create the RFE with a LogisticRegression estimator and 3 features to select\n# rfe = RFE(estimator=LogisticRegression(), n_features_to_select=10, verbose=1)\n# \n# # Fits the eliminator to the data\n# rfe.fit(X_train, y_train)\n# \n# # Print the features and their ranking (high = dropped early on)\n# print(dict(zip(X.columns, rfe.ranking_)))\n# \n# # Print the features that are not eliminated\n# print(X.columns[rfe.support_])\n# \n# # Calculates the test set accuracy\n# acc = accuracy_score(y_test, rfe.predict(X_test))\n# print(\"{0:.1%} accuracy on test set.\".format(acc)) \n\n\n\n\n## https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py\n\n## https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}